{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "time_len = 200\n",
    "\n",
    "def generate_demonstrations(time_len = 200, params = None, title = None):\n",
    "    def dist_generator(d, x, param, noise = 0):\n",
    "        f = (math.exp(-x**2/(2.*param[0]**2))/(math.sqrt(2*math.pi)*param[0]))+param[1]\n",
    "        return f+(noise*(np.random.rand()-0.5)/100.)\n",
    "    def sinx(x, frequency, amplitude, phase):\n",
    "        return amplitude * math.sin(2 * math.pi * frequency * x + phase)\n",
    "    \n",
    "    num_demo = 32\n",
    "    frequencies = [1]\n",
    "    amplitudes_0 = np.linspace(0.5,0.67,num_demo//2)\n",
    "    amplitudes_1 = np.linspace(0.84,1.0,num_demo//2)\n",
    "    amplitudes = np.concatenate((amplitudes_0, amplitudes_1))\n",
    "    phases = [0] \n",
    "\n",
    "    #fig = plt.figure(figsize=(5,5))\n",
    "    x = np.linspace(-0.5,0.5,time_len)\n",
    "    times = np.zeros((num_demo,time_len,1))\n",
    "    times[:] = x.reshape((1,time_len,1))+0.5\n",
    "    values = np.zeros((num_demo,time_len,1))\n",
    "    for d in range(num_demo):\n",
    "        for i in range(time_len):\n",
    "            values[d,i] = sinx(times[0][i][0], frequencies[d % len(frequencies)], amplitudes[d % len(amplitudes)], phases[d % len(phases)])\n",
    "        plt.plot(times[d], values[d], color=\"black\", alpha=0.05)\n",
    "    #plt.title(title+' Demonstrations')\n",
    "    #plt.ylabel('Y')\n",
    "    #plt.xlabel('time (t)')\n",
    "    plt.show()\n",
    "    return times, values\n",
    "\n",
    "# gets random number of random obs. points from a random trajectory. Also gets a \n",
    "# random target (x,y) from the same trajectory\n",
    "def get_training_sample():\n",
    "    \n",
    "    n = np.random.randint(0,OBS_MAX)+1\n",
    "    d = np.random.randint(0, d_N)\n",
    "    \n",
    "    observations = np.zeros((n,d_x+d_y)) \n",
    "    target_X = np.zeros((1,d_x))\n",
    "    target_Y = np.zeros((1,d_y))\n",
    "    \n",
    "    perm = np.random.permutation(time_len)\n",
    "    observations[:,:d_x] = X[d,perm[:n]]\n",
    "    observations[:,d_x:d_x+d_y] = Y[d,perm[:n]]\n",
    "    target_X[0] = X[d,perm[n]]\n",
    "    target_Y[0] = Y[d,perm[n]]\n",
    "    return torch.from_numpy(observations), torch.from_numpy(target_X), torch.from_numpy(target_Y)\n",
    "\n",
    "def log_prob_loss(output, y_target): \n",
    "    mean, std = output.chunk(2, dim=-1)\n",
    "    std = F.softplus(std)\n",
    "    dist = D.Independent(D.Normal(loc=mean, scale=std), 1)  # (d_y distributions)\n",
    "    return -torch.mean(dist.log_prob(y_target)) \n",
    "\n",
    "class CNMP(nn.Module):\n",
    "    def __init__(self, d_x, d_y):\n",
    "        super(CNMP, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(d_x + d_y, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_x + 128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 2*d_y)\n",
    "        )\n",
    "    def forward(self, obs, x_tar): # obs is (n, d_x + d_y)\n",
    "\n",
    "        r = self.encoder(obs) # (n,128)\n",
    "        r_avg = torch.mean(r, 0) # (1,128)\n",
    "        r_avg = r_avg.repeat(x_tar.shape[0],1) # Duplicating general representation for every target_t\n",
    "\n",
    "        concat = torch.cat((r_avg, x_tar), dim=-1)\n",
    "        output = self.decoder(concat) # (2*d_y,)\n",
    "        return output\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "X, Y = generate_demonstrations(time_len=200, params=np.array([[0.6,-0.1],[0.5,-0.23],[0.4,-0.43],[-0.6,0.1],[-0.5,0.23],[-0.4,0.43]]), title='Training')\n",
    "\n",
    "\n",
    "print('training X ', X.shape)\n",
    "print('training Y ',Y.shape)\n",
    "\n",
    "OBS_MAX = 5\n",
    "d_x = X.shape[-1]\n",
    "d_y = Y.shape[-1]\n",
    "d_N = X.shape[0]\n",
    "\n",
    "model = CNMP(d_x, d_y).double()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "#print(model.eval())\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(300000):\n",
    "\n",
    "    obs, x_tar, y_tar = get_training_sample()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(obs, x_tar)\n",
    "    loss = log_prob_loss(output, y_tar)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print('loss ', loss.item())\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        #print('iteration ', i)\n",
    "        print('loss ', loss.item())\n",
    "        losses.append(loss.item())\n",
    "        if loss.item() == min(losses):\n",
    "            print(\"Saving model\")\n",
    "            torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cProfile import label\n",
    "\n",
    "\n",
    "def predict_model(observations, target_X, plot = True):\n",
    "    d_N = X.shape[0]\n",
    "    predicted_Y = np.zeros((time_len,d_y))\n",
    "    predicted_std = np.zeros((time_len,d_y))\n",
    "    with torch.no_grad():\n",
    "        prediction = model(torch.from_numpy(observations),torch.from_numpy(target_X)).numpy()\n",
    "    predicted_Y = prediction[:,:d_y]\n",
    "    predicted_std = np.log(1+np.exp(prediction[:,d_y:]))\n",
    "    if plot: # We highly recommend that you customize your own plot function, but you can use this function as default\n",
    "        for i in range(d_y): #for every feature in Y vector we are plotting training data and its prediction\n",
    "            fig = plt.figure(figsize=(10,10))\n",
    "            for j in range(d_N):\n",
    "                if j == 0:\n",
    "                    plt.plot(X[j,:,0],Y[j,:,i], color=\"black\", alpha=0.05, label=\"Demonstrations\") # assuming X[j,:,0] is time\n",
    "                else:\n",
    "                    plt.plot(X[j,:,0],Y[j,:,i], color=\"black\", alpha=0.05)\n",
    "            plt.title(\"CNMP Prediction\")\n",
    "            plt.plot(X[j,:,0],predicted_Y[:,i],color='red', label='Predicted')\n",
    "            plt.errorbar(X[j,:,0],predicted_Y[:,i],yerr=predicted_std[:,i],color = 'black',alpha=0.1)\n",
    "            plt.scatter(observations[:,0],observations[:,d_x+i],marker=\"X\",color='black', label='Observations')\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "            plt.show()  \n",
    "    return predicted_Y, predicted_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CNMP(d_x, d_y).double()\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "t = 0.25\n",
    "min_y = np.min(Y[:, int(t * time_len)])\n",
    "max_y = np.max(Y[:, int(t * time_len)])\n",
    "interval_y = np.linspace(min_y-0.1, max_y+0.1, 20)\n",
    "predicted_Y, predicted_std = predict_model(np.array([[0.25, interval_y[10]]]),X[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
